{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10190173,"sourceType":"datasetVersion","datasetId":6295853},{"sourceId":10258800,"sourceType":"datasetVersion","datasetId":6346139}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# MODELE FASTER-CNN","metadata":{"id":"CZ4L7OiHRlZ2"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nimport math\nimport os\nimport warnings\n\n\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n","metadata":{"id":"fLurb_qvRlZ5","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T14:10:14.313451Z","iopub.execute_input":"2025-01-09T14:10:14.313831Z","iopub.status.idle":"2025-01-09T14:10:18.457658Z","shell.execute_reply.started":"2025-01-09T14:10:14.313772Z","shell.execute_reply":"2025-01-09T14:10:18.456799Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# DATASET Transformation","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport random\n\nimport torch\nimport torchvision\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom torch.utils.data.dataset import Dataset\nimport pandas as pd\n\nimport pandas as pd\nimport os\nimport glob\nfrom tqdm import tqdm\n\nclasses = ['Maize', 'Sugar beet', 'Soy', 'Sunflower', 'Potato', 'Pea', 'Bean', 'Pumpkin', 'Weed']\n\ndef load_images_and_anns_from_csv(im_dir, ann_dir):\n    \"\"\"\n    Method to load images and their annotations from CSV files.\n    Each CSV file corresponds to an image and contains bounding box and label information.\n\n    :param im_dir: Path to the directory containing images.\n    :param ann_dir: Path to the directory containing annotation CSV files.\n    :return: List of dictionaries containing image info and detections.\n    \"\"\"\n    im_infos = []\n    for ann_file in tqdm(glob.glob(os.path.join(ann_dir, '*.csv'))):\n        im_info = {}\n        im_info['img_id'] = os.path.basename(ann_file).split('.csv')[0]\n        im_info['filename'] = os.path.join(im_dir, '{}.jpg'.format(im_info['img_id']))\n\n        if not os.path.exists(im_info['filename']):\n            print(f\"Warning: Image file {im_info['filename']} not found, skipping.\")\n            continue\n\n        try:\n            # Load annotation CSV without headers\n            annotations = pd.read_csv(ann_file, header=None)\n\n            # Check if the CSV is empty\n            if annotations.empty:\n                # print(f\"Warning: Annotation file {ann_file} is empty, skipping.\")\n                continue\n        except pd.errors.EmptyDataError:\n            # print(f\"Warning: Annotation file {ann_file} could not be parsed, skipping.\")\n            continue\n        except Exception as e:\n            print(f\"Error loading {ann_file}: {e}\")\n            continue\n\n        detections = []\n        for _, row in annotations.iterrows():\n            det = {}\n            bbox = [\n                int(row[0]),  # Left\n                int(row[1]),  # Top\n                int(row[2]),  # Right\n                int(row[3])   # Bottom\n            ]\n            label = int(row[4])  # Label ID\n            det['label'] = label\n            det['bbox'] = bbox\n            detections.append(det)\n\n        if not detections:\n            print(f\"Warning: No valid detections found in {ann_file}, skipping.\")\n            continue\n\n        im_info['detections'] = detections\n        im_infos.append(im_info)\n\n    print(f'Total {len(im_infos)} images with annotations found.')\n    return im_infos\n\n    \nclass CSVDetectionDataset():\n    def __init__(self, split, im_dir, ann_dir, classes):\n        \"\"\"\n        Dataset class for images and annotations stored in CSV files.\n\n        :param split: Dataset split (e.g., 'train' or 'val').\n        :param im_dir: Directory containing images.\n        :param ann_dir: Directory containing annotation CSV files.\n        :param classes: List of class names.\n        \"\"\"\n        self.split = split\n        self.im_dir = im_dir\n        self.ann_dir = ann_dir\n\n        # Liste des classes sans \"background\"\n        classes = ['Maize', 'Sugar beet', 'Soy', 'Sunflower', 'Potato', 'Pea', 'Bean', 'Pumpkin', 'Weed']\n        # Inclure background si nécessaire\n        classes = ['background'] + classes \n\n        # Créer le mappage des labels\n        self.label2idx = {classes[idx]: idx for idx in range(len(classes))}\n        self.idx2label = {idx: classes[idx] for idx in range(len(classes))}\n\n        print(self.idx2label)\n\n        # Charger les informations d'images et d'annotations\n        self.images_info = load_images_and_anns_from_csv(im_dir, ann_dir)\n\n    def __len__(self):\n        return len(self.images_info)\n\n    def __getitem__(self, index):\n        im_info = self.images_info[index]\n\n        # Charger l'image\n        im = Image.open(im_info['filename'])\n\n        # Appliquer un flip horizontal aléatoire pour l'entraînement\n        to_flip = False\n        if self.split == 'train' and random.random() < 0.5:\n            to_flip = True\n            im = im.transpose(Image.FLIP_LEFT_RIGHT)\n\n        # Convertir l'image en tensor\n        im_tensor = torchvision.transforms.ToTensor()(im)\n\n        # Préparer les cibles\n        targets = {}\n        if im_info['detections']:\n            targets['bboxes'] = torch.as_tensor([detection['bbox'] for detection in im_info['detections']])\n            targets['labels'] = torch.as_tensor([detection['label'] for detection in im_info['detections']])\n\n            # Mettre à jour les labels de 0 à 8 vers 1 à 9 et gérer le 255\n            targets['labels'] = self.update_labels(targets['labels'])\n        else:\n            # Images sans annotations\n            targets['bboxes'] = torch.empty((0, 4), dtype=torch.float32)\n            targets['labels'] = torch.empty((0,), dtype=torch.int64)\n\n        # Ajuster les boîtes de délimitation pour les images retournées\n        if to_flip:\n            for idx, box in enumerate(targets['bboxes']):\n                x1, y1, x2, y2 = box\n                w = x2 - x1\n                im_w = im_tensor.shape[-1]\n                x1 = im_w - x1 - w\n                x2 = x1 + w\n                targets['bboxes'][idx] = torch.as_tensor([x1, y1, x2, y2])\n\n        return im_tensor, targets, im_info['filename']\n\n    def update_labels(self, labels):\n    \n        updated_labels = labels.clone()  # Créer une copie pour ne pas modifier l'original\n\n        # Décaler tous les labels de 0 à 8 vers 1 à 9\n        updated_labels = updated_labels + 1  \n\n        # Si le label est 255, le remplacer par 10 (ou une autre valeur spéciale)\n        updated_labels[updated_labels == 256] = -1\n        \n        return updated_labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T14:10:18.458883Z","iopub.execute_input":"2025-01-09T14:10:18.459333Z","iopub.status.idle":"2025-01-09T14:10:18.838524Z","shell.execute_reply.started":"2025-01-09T14:10:18.459301Z","shell.execute_reply":"2025-01-09T14:10:18.837743Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def get_iou(boxes1, boxes2):\n    r\"\"\"\n    IOU between two sets of boxes\n    :param boxes1: (Tensor of shape N x 4)\n    :param boxes2: (Tensor of shape M x 4)\n    :return: IOU matrix of shape N x M\n    \"\"\"\n    # Area of boxes (x2-x1)*(y2-y1)\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])  # (N,)\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])  # (M,)\n\n    # Get top left x1,y1 coordinate\n    x_left = torch.max(boxes1[:, None, 0], boxes2[:, 0])  # (N, M)\n    y_top = torch.max(boxes1[:, None, 1], boxes2[:, 1])  # (N, M)\n\n    # Get bottom right x2,y2 coordinate\n    x_right = torch.min(boxes1[:, None, 2], boxes2[:, 2])  # (N, M)\n    y_bottom = torch.min(boxes1[:, None, 3], boxes2[:, 3])  # (N, M)\n\n    intersection_area = (x_right - x_left).clamp(min=0) * (y_bottom - y_top).clamp(min=0)  # (N, M)\n    union = area1[:, None] + area2 - intersection_area  # (N, M)\n    iou = intersection_area / union  # (N, M)\n    return iou","metadata":{"id":"TTJ2dCgkRlZ7","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T14:10:18.840199Z","iopub.execute_input":"2025-01-09T14:10:18.840616Z","iopub.status.idle":"2025-01-09T14:10:18.846967Z","shell.execute_reply.started":"2025-01-09T14:10:18.840593Z","shell.execute_reply":"2025-01-09T14:10:18.845909Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def boxes_to_transformation_targets(ground_truth_boxes, anchors_or_proposals):\n    r\"\"\"\n    Given all anchor boxes or proposals in image and their respective\n    ground truth assignments, we use the x1,y1,x2,y2 coordinates of them\n    to get tx,ty,tw,th transformation targets for all anchor boxes or proposals\n    :param ground_truth_boxes: (anchors_or_proposals_in_image, 4)\n        Ground truth box assignments for the anchors/proposals\n    :param anchors_or_proposals: (anchors_or_proposals_in_image, 4) Anchors/Proposal boxes\n    :return: regression_targets: (anchors_or_proposals_in_image, 4) transformation targets tx,ty,tw,th\n        for all anchors/proposal boxes\n    \"\"\"\n\n    # Get center_x,center_y,w,h from x1,y1,x2,y2 for anchors\n    widths = anchors_or_proposals[:, 2] - anchors_or_proposals[:, 0]\n    heights = anchors_or_proposals[:, 3] - anchors_or_proposals[:, 1]\n    center_x = anchors_or_proposals[:, 0] + 0.5 * widths\n    center_y = anchors_or_proposals[:, 1] + 0.5 * heights\n\n    # Get center_x,center_y,w,h from x1,y1,x2,y2 for gt boxes\n    gt_widths = ground_truth_boxes[:, 2] - ground_truth_boxes[:, 0]\n    gt_heights = ground_truth_boxes[:, 3] - ground_truth_boxes[:, 1]\n    gt_center_x = ground_truth_boxes[:, 0] + 0.5 * gt_widths\n    gt_center_y = ground_truth_boxes[:, 1] + 0.5 * gt_heights\n\n    targets_dx = (gt_center_x - center_x) / widths\n    targets_dy = (gt_center_y - center_y) / heights\n    targets_dw = torch.log(gt_widths / widths)\n    targets_dh = torch.log(gt_heights / heights)\n    regression_targets = torch.stack((targets_dx, targets_dy, targets_dw, targets_dh), dim=1)\n    return regression_targets","metadata":{"id":"Tejc0CeXRlZ8","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T14:10:18.847746Z","iopub.execute_input":"2025-01-09T14:10:18.847974Z","iopub.status.idle":"2025-01-09T14:10:18.868015Z","shell.execute_reply.started":"2025-01-09T14:10:18.847955Z","shell.execute_reply":"2025-01-09T14:10:18.867213Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def apply_regression_pred_to_anchors_or_proposals(box_transform_pred, anchors_or_proposals):\n    r\"\"\"\n    Given the transformation parameter predictions for all\n    input anchors or proposals, transform them accordingly\n    to generate predicted proposals or predicted boxes\n    :param box_transform_pred: (num_anchors_or_proposals, num_classes, 4)\n    :param anchors_or_proposals: (num_anchors_or_proposals, 4)\n    :return pred_boxes: (num_anchors_or_proposals, num_classes, 4)\n    \"\"\"\n    box_transform_pred = box_transform_pred.reshape(\n        box_transform_pred.size(0), -1, 4)\n\n    # Get cx, cy, w, h from x1,y1,x2,y2\n    w = anchors_or_proposals[:, 2] - anchors_or_proposals[:, 0]\n    h = anchors_or_proposals[:, 3] - anchors_or_proposals[:, 1]\n    center_x = anchors_or_proposals[:, 0] + 0.5 * w\n    center_y = anchors_or_proposals[:, 1] + 0.5 * h\n\n    dx = box_transform_pred[..., 0]\n    dy = box_transform_pred[..., 1]\n    dw = box_transform_pred[..., 2]\n    dh = box_transform_pred[..., 3]\n    # dh -> (num_anchors_or_proposals, num_classes)\n\n    # Prevent sending too large values into torch.exp()\n    dw = torch.clamp(dw, max=math.log(1000.0 / 16))\n    dh = torch.clamp(dh, max=math.log(1000.0 / 16))\n\n    pred_center_x = dx * w[:, None] + center_x[:, None]\n    pred_center_y = dy * h[:, None] + center_y[:, None]\n    pred_w = torch.exp(dw) * w[:, None]\n    pred_h = torch.exp(dh) * h[:, None]\n    # pred_center_x -> (num_anchors_or_proposals, num_classes)\n\n    pred_box_x1 = pred_center_x - 0.5 * pred_w\n    pred_box_y1 = pred_center_y - 0.5 * pred_h\n    pred_box_x2 = pred_center_x + 0.5 * pred_w\n    pred_box_y2 = pred_center_y + 0.5 * pred_h\n\n    pred_boxes = torch.stack((\n        pred_box_x1,\n        pred_box_y1,\n        pred_box_x2,\n        pred_box_y2),\n        dim=2)\n    # pred_boxes -> (num_anchors_or_proposals, num_classes, 4)\n    return pred_boxes","metadata":{"id":"VRsJBwpxRlZ9","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T14:10:18.868907Z","iopub.execute_input":"2025-01-09T14:10:18.869217Z","iopub.status.idle":"2025-01-09T14:10:18.888984Z","shell.execute_reply.started":"2025-01-09T14:10:18.869191Z","shell.execute_reply":"2025-01-09T14:10:18.888218Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def sample_positive_negative(labels, positive_count, total_count):\n    # Sample positive and negative proposals\n    positive = torch.where(labels > 0)[0]\n    negative = torch.where(labels == 0)[0]\n    num_pos = positive_count\n    num_pos = min(positive.numel(), num_pos)\n    num_neg = total_count - num_pos\n    num_neg = min(negative.numel(), num_neg)\n    perm_positive_idxs = torch.randperm(positive.numel(),\n                                        device=positive.device)[:num_pos]\n    perm_negative_idxs = torch.randperm(negative.numel(),\n                                        device=negative.device)[:num_neg]\n    pos_idxs = positive[perm_positive_idxs]\n    neg_idxs = negative[perm_negative_idxs]\n    sampled_pos_idx_mask = torch.zeros_like(labels, dtype=torch.bool)\n    sampled_neg_idx_mask = torch.zeros_like(labels, dtype=torch.bool)\n    sampled_pos_idx_mask[pos_idxs] = True\n    sampled_neg_idx_mask[neg_idxs] = True\n    return sampled_neg_idx_mask, sampled_pos_idx_mask","metadata":{"id":"gvAgsg1xRlZ-","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T14:10:19.187699Z","iopub.execute_input":"2025-01-09T14:10:19.188025Z","iopub.status.idle":"2025-01-09T14:10:19.193676Z","shell.execute_reply.started":"2025-01-09T14:10:19.187998Z","shell.execute_reply":"2025-01-09T14:10:19.192815Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def clamp_boxes_to_image_boundary(boxes, image_shape):\n    boxes_x1 = boxes[..., 0]\n    boxes_y1 = boxes[..., 1]\n    boxes_x2 = boxes[..., 2]\n    boxes_y2 = boxes[..., 3]\n    height, width = image_shape[-2:]\n    boxes_x1 = boxes_x1.clamp(min=0, max=width)\n    boxes_x2 = boxes_x2.clamp(min=0, max=width)\n    boxes_y1 = boxes_y1.clamp(min=0, max=height)\n    boxes_y2 = boxes_y2.clamp(min=0, max=height)\n    boxes = torch.cat((\n        boxes_x1[..., None],\n        boxes_y1[..., None],\n        boxes_x2[..., None],\n        boxes_y2[..., None]),\n        dim=-1)\n    return boxes","metadata":{"id":"JduTeD6uRlZ_","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T14:10:19.768303Z","iopub.execute_input":"2025-01-09T14:10:19.768580Z","iopub.status.idle":"2025-01-09T14:10:19.773893Z","shell.execute_reply.started":"2025-01-09T14:10:19.768558Z","shell.execute_reply":"2025-01-09T14:10:19.772886Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def transform_boxes_to_original_size(boxes, new_size, original_size):\n    r\"\"\"\n    Boxes are for resized image (min_size=600, max_size=1000).\n    This method converts the boxes to whatever dimensions\n    the image was before resizing\n    :param boxes:\n    :param new_size:\n    :param original_size:\n    :return:\n    \"\"\"\n    ratios = [\n        torch.tensor(s_orig, dtype=torch.float32, device=boxes.device)\n        / torch.tensor(s, dtype=torch.float32, device=boxes.device)\n        for s, s_orig in zip(new_size, original_size)\n    ]\n    ratio_height, ratio_width = ratios\n    xmin, ymin, xmax, ymax = boxes.unbind(1)\n    xmin = xmin * ratio_width\n    xmax = xmax * ratio_width\n    ymin = ymin * ratio_height\n    ymax = ymax * ratio_height\n    return torch.stack((xmin, ymin, xmax, ymax), dim=1)","metadata":{"id":"UP_v-3DBRlZ_","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T14:10:20.385090Z","iopub.execute_input":"2025-01-09T14:10:20.385380Z","iopub.status.idle":"2025-01-09T14:10:20.390549Z","shell.execute_reply.started":"2025-01-09T14:10:20.385359Z","shell.execute_reply":"2025-01-09T14:10:20.389647Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class RegionProposalNetwork(nn.Module):\n    r\"\"\"\n    RPN with following layers on the feature map\n        1. 3x3 conv layer followed by Relu\n        2. 1x1 classification conv with num_anchors(num_scales x num_aspect_ratios) output channels\n        3. 1x1 classification conv with 4 x num_anchors output channels\n\n    Classification is done via one value indicating probability of foreground\n    with sigmoid applied during inference\n    \"\"\"\n\n    def __init__(self, in_channels, scales, aspect_ratios, model_config):\n        super(RegionProposalNetwork, self).__init__()\n        self.scales = scales\n        self.low_iou_threshold = model_config['rpn_bg_threshold']\n        self.high_iou_threshold = model_config['rpn_fg_threshold']\n        self.rpn_nms_threshold = model_config['rpn_nms_threshold']\n        self.rpn_batch_size = model_config['rpn_batch_size']\n        self.rpn_pos_count = int(model_config['rpn_pos_fraction'] * self.rpn_batch_size)\n        self.rpn_topk = model_config['rpn_train_topk'] if self.training else model_config['rpn_test_topk']\n        self.rpn_prenms_topk = model_config['rpn_train_prenms_topk'] if self.training \\\n            else model_config['rpn_test_prenms_topk']\n        self.aspect_ratios = aspect_ratios\n        self.num_anchors = len(self.scales) * len(self.aspect_ratios)\n\n        # 3x3 conv layer\n        self.rpn_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n\n        # 1x1 classification conv layer\n        self.cls_layer = nn.Conv2d(in_channels, self.num_anchors, kernel_size=1, stride=1)\n\n        # 1x1 regression\n        self.bbox_reg_layer = nn.Conv2d(in_channels, self.num_anchors * 4, kernel_size=1, stride=1)\n\n        for layer in [self.rpn_conv, self.cls_layer, self.bbox_reg_layer]:\n            torch.nn.init.normal_(layer.weight, std=0.01)\n            torch.nn.init.constant_(layer.bias, 0)\n\n    def generate_anchors(self, image, feat):\n        r\"\"\"\n        Method to generate anchors. First we generate one set of zero-centred anchors\n        using the scales and aspect ratios provided.\n        We then generate shift values in x,y axis for all featuremap locations.\n        The single zero centred anchors generated are replicated and shifted accordingly\n        to generate anchors for all feature map locations.\n        Note that these anchors are generated such that their centre is top left corner of the\n        feature map cell rather than the centre of the feature map cell.\n        :param image: (N, C, H, W) tensor\n        :param feat: (N, C_feat, H_feat, W_feat) tensor\n        :return: anchor boxes of shape (H_feat * W_feat * num_anchors_per_location, 4)\n        \"\"\"\n        grid_h, grid_w = feat.shape[-2:]\n        image_h, image_w = image.shape[-2:]\n\n        # For the vgg16 case stride would be 16 for both h and w\n        stride_h = torch.tensor(image_h // grid_h, dtype=torch.int64, device=feat.device)\n        stride_w = torch.tensor(image_w // grid_w, dtype=torch.int64, device=feat.device)\n\n        scales = torch.as_tensor(self.scales, dtype=feat.dtype, device=feat.device)\n        aspect_ratios = torch.as_tensor(self.aspect_ratios, dtype=feat.dtype, device=feat.device)\n\n        # Assuming anchors of scale 128 sq pixels\n        # For 1:1 it would be (128, 128) -> area=16384\n        # For 2:1 it would be (181.02, 90.51) -> area=16384\n        # For 1:2 it would be (90.51, 181.02) -> area=16384\n\n        # The below code ensures h/w = aspect_ratios and h*w=1\n        h_ratios = torch.sqrt(aspect_ratios)\n        w_ratios = 1 / h_ratios\n\n        # Now we will just multiply h and w with scale(example 128)\n        # to make h*w = 128 sq pixels and h/w = aspect_ratios\n        # This gives us the widths and heights of all anchors\n        # which we need to replicate at all locations\n        ws = (w_ratios[:, None] * scales[None, :]).view(-1)\n        hs = (h_ratios[:, None] * scales[None, :]).view(-1)\n\n        # Now we make all anchors zero centred\n        # So x1, y1, x2, y2 = -w/2, -h/2, w/2, h/2\n        base_anchors = torch.stack([-ws, -hs, ws, hs], dim=1) / 2\n        base_anchors = base_anchors.round()\n\n        # Get the shifts in x axis (0, 1,..., W_feat-1) * stride_w\n        shifts_x = torch.arange(0, grid_w, dtype=torch.int32, device=feat.device) * stride_w\n\n        # Get the shifts in x axis (0, 1,..., H_feat-1) * stride_h\n        shifts_y = torch.arange(0, grid_h, dtype=torch.int32, device=feat.device) * stride_h\n\n        # Create a grid using these shifts\n        shifts_y, shifts_x = torch.meshgrid(shifts_y, shifts_x, indexing=\"ij\")\n        # shifts_x -> (H_feat, W_feat)\n        # shifts_y -> (H_feat, W_feat)\n\n        shifts_x = shifts_x.reshape(-1)\n        shifts_y = shifts_y.reshape(-1)\n        # Setting shifts for x1 and x2(same as shifts_x) and y1 and y2(same as shifts_y)\n        shifts = torch.stack((shifts_x, shifts_y, shifts_x, shifts_y), dim=1)\n        # shifts -> (H_feat * W_feat, 4)\n\n        # base_anchors -> (num_anchors_per_location, 4)\n        # shifts -> (H_feat * W_feat, 4)\n        # Add these shifts to each of the base anchors\n        anchors = (shifts.view(-1, 1, 4) + base_anchors.view(1, -1, 4))\n        # anchors -> (H_feat * W_feat, num_anchors_per_location, 4)\n        anchors = anchors.reshape(-1, 4)\n        # anchors -> (H_feat * W_feat * num_anchors_per_location, 4)\n        return anchors\n\n    def assign_targets_to_anchors(self, anchors, gt_boxes):\n        r\"\"\"\n        For each anchor assign a ground truth box based on the IOU.\n        Also creates classification labels to be used for training\n        label=1 for anchors where maximum IOU with a gtbox > high_iou_threshold\n        label=0 for anchors where maximum IOU with a gtbox < low_iou_threshold\n        label=-1 for anchors where maximum IOU with a gtbox between (low_iou_threshold, high_iou_threshold)\n        :param anchors: (num_anchors_in_image, 4) all anchor boxes\n        :param gt_boxes: (num_gt_boxes_in_image, 4) all ground truth boxes\n        :return:\n            label: (num_anchors_in_image) {-1/0/1}\n            matched_gt_boxes: (num_anchors_in_image, 4) coordinates of assigned gt_box to each anchor\n                Even background/to_be_ignored anchors will be assigned some ground truth box.\n                It's fine, we will use label to differentiate those instances later\n        \"\"\"\n\n        # Get (gt_boxes, num_anchors_in_image) IOU matrix\n        iou_matrix = get_iou(gt_boxes, anchors)\n\n        # For each anchor get the gt box index with maximum overlap\n        best_match_iou, best_match_gt_idx = iou_matrix.max(dim=0)\n        # best_match_gt_idx -> (num_anchors_in_image)\n\n        # This copy of best_match_gt_idx will be needed later to\n        # add low quality matches\n        best_match_gt_idx_pre_thresholding = best_match_gt_idx.clone()\n\n        # Based on threshold, update the values of best_match_gt_idx\n        # For anchors with highest IOU < low_threshold update to be -1\n        # For anchors with highest IOU between low_threshold & high threshold update to be -2\n        below_low_threshold = best_match_iou < self.low_iou_threshold\n        between_thresholds = (best_match_iou >= self.low_iou_threshold) & (best_match_iou < self.high_iou_threshold)\n        best_match_gt_idx[below_low_threshold] = -1\n        best_match_gt_idx[between_thresholds] = -2\n\n        # Add low quality anchor boxes, if for a given ground truth box, these are the ones\n        # that have highest IOU with that gt box\n\n        # For each gt box, get the maximum IOU value amongst all anchors\n        best_anchor_iou_for_gt, _ = iou_matrix.max(dim=1)\n        # best_anchor_iou_for_gt -> (num_gt_boxes_in_image)\n\n        # For each gt box get those anchors\n        # which have this same IOU as present in best_anchor_iou_for_gt\n        # This is to ensure if 10 anchors all have the same IOU value,\n        # which is equal to the highest IOU that this gt box has with any anchor\n        # then we get all these 10 anchors\n        gt_pred_pair_with_highest_iou = torch.where(iou_matrix == best_anchor_iou_for_gt[:, None])\n        # gt_pred_pair_with_highest_iou -> [0, 0, 0, 1, 1, 1], [8896,  8905,  8914, 10472, 10805, 11138]\n        # This means that anchors at the first 3 indexes have an IOU with gt box at index 0\n        # which is equal to the highest IOU that this gt box has with ANY anchor\n        # Similarly anchor at last three indexes(10472, 10805, 11138) have an IOU with gt box at index 1\n        # which is equal to the highest IOU that this gt box has with ANY anchor\n        # These 6 anchor indexes will also be added as positive anchors\n\n        # Get all the anchors indexes to update\n        pred_inds_to_update = gt_pred_pair_with_highest_iou[1]\n\n        # Update the matched gt index for all these anchors with whatever was the best gt box\n        # prior to thresholding\n        best_match_gt_idx[pred_inds_to_update] = best_match_gt_idx_pre_thresholding[pred_inds_to_update]\n\n        # best_match_gt_idx is either a valid index for all anchors or -1(background) or -2(to be ignored)\n        # Clamp this so that the best_match_gt_idx is a valid non-negative index\n        # At this moment the -1 and -2 labelled anchors will be mapped to the 0th gt box\n        matched_gt_boxes = gt_boxes[best_match_gt_idx.clamp(min=0)]\n\n        # Set all foreground anchor labels as 1\n        labels = best_match_gt_idx >= 0\n        labels = labels.to(dtype=torch.float32)\n\n        # Set all background anchor labels as 0\n        background_anchors = best_match_gt_idx == -1\n        labels[background_anchors] = 0\n\n        # Set all to be ignored anchor labels as -1\n        ignored_anchors = best_match_gt_idx == -2\n        labels[ignored_anchors] = -1.0\n        # Later for classification we will only pick labels which have > 0 label\n\n        return labels, matched_gt_boxes\n\n    def filter_proposals(self, proposals, cls_scores, image_shape):\n        r\"\"\"\n        This method does three kinds of filtering/modifications\n        1. Pre NMS topK filtering\n        2. Make proposals valid by clamping coordinates(0, width/height)\n        2. Small Boxes filtering based on width and height\n        3. NMS\n        4. Post NMS topK filtering\n        :param proposals: (num_anchors_in_image, 4)\n        :param cls_scores: (num_anchors_in_image, 4) these are cls logits\n        :param image_shape: resized image shape needed to clip proposals to image boundary\n        :return: proposals and cls_scores: (num_filtered_proposals, 4) and (num_filtered_proposals)\n        \"\"\"\n        # Pre NMS Filtering\n        cls_scores = cls_scores.reshape(-1)\n        cls_scores = torch.sigmoid(cls_scores)\n        _, top_n_idx = cls_scores.topk(min(self.rpn_prenms_topk, len(cls_scores)))\n\n        cls_scores = cls_scores[top_n_idx]\n        proposals = proposals[top_n_idx]\n        ##################\n\n        # Clamp boxes to image boundary\n        proposals = clamp_boxes_to_image_boundary(proposals, image_shape)\n        ####################\n\n        # Small boxes based on width and height filtering\n        min_size = 16\n        ws, hs = proposals[:, 2] - proposals[:, 0], proposals[:, 3] - proposals[:, 1]\n        keep = (ws >= min_size) & (hs >= min_size)\n        keep = torch.where(keep)[0]\n        proposals = proposals[keep]\n        cls_scores = cls_scores[keep]\n        ####################\n\n        # NMS based on objectness scores\n        keep_mask = torch.zeros_like(cls_scores, dtype=torch.bool)\n        keep_indices = torch.ops.torchvision.nms(proposals, cls_scores, self.rpn_nms_threshold)\n        keep_mask[keep_indices] = True\n        keep_indices = torch.where(keep_mask)[0]\n        # Sort by objectness\n        post_nms_keep_indices = keep_indices[cls_scores[keep_indices].sort(descending=True)[1]]\n\n        # Post NMS topk filtering\n        proposals, cls_scores = (proposals[post_nms_keep_indices[:self.rpn_topk]],\n                                 cls_scores[post_nms_keep_indices[:self.rpn_topk]])\n\n        return proposals, cls_scores\n\n    def forward(self, image, feat, target=None):\n        r\"\"\"\n        Main method for RPN does the following:\n        1. Call RPN specific conv layers to generate classification and\n            bbox transformation predictions for anchors\n        2. Generate anchors for entire image\n        3. Transform generated anchors based on predicted bbox transformation to generate proposals\n        4. Filter proposals\n        5. For training additionally we do the following:\n            a. Assign target ground truth labels and boxes to each anchors\n            b. Sample positive and negative anchors\n            c. Compute classification loss using sampled pos/neg anchors\n            d. Compute Localization loss using sampled pos anchors\n        :param image:\n        :param feat:\n        :param target:\n        :return:\n        \"\"\"\n        # Call RPN layers\n        rpn_feat = nn.ReLU()(self.rpn_conv(feat))\n        cls_scores = self.cls_layer(rpn_feat)\n        box_transform_pred = self.bbox_reg_layer(rpn_feat)\n\n        # Generate anchors\n        anchors = self.generate_anchors(image, feat)\n\n        # Reshape classification scores to be (Batch Size * H_feat * W_feat * Number of Anchors Per Location, 1)\n        # cls_score -> (Batch_Size, Number of Anchors per location, H_feat, W_feat)\n        number_of_anchors_per_location = cls_scores.size(1)\n        cls_scores = cls_scores.permute(0, 2, 3, 1)\n        cls_scores = cls_scores.reshape(-1, 1)\n        # cls_score -> (Batch_Size*H_feat*W_feat*Number of Anchors per location, 1)\n\n        # Reshape bbox predictions to be (Batch Size * H_feat * W_feat * Number of Anchors Per Location, 4)\n        # box_transform_pred -> (Batch_Size, Number of Anchors per location*4, H_feat, W_feat)\n        box_transform_pred = box_transform_pred.view(\n            box_transform_pred.size(0),\n            number_of_anchors_per_location,\n            4,\n            rpn_feat.shape[-2],\n            rpn_feat.shape[-1])\n        box_transform_pred = box_transform_pred.permute(0, 3, 4, 1, 2)\n        box_transform_pred = box_transform_pred.reshape(-1, 4)\n        # box_transform_pred -> (Batch_Size*H_feat*W_feat*Number of Anchors per location, 4)\n\n        # Transform generated anchors according to box transformation prediction\n        proposals = apply_regression_pred_to_anchors_or_proposals(\n            box_transform_pred.detach().reshape(-1, 1, 4),\n            anchors)\n        proposals = proposals.reshape(proposals.size(0), 4)\n        ######################\n\n        proposals, scores = self.filter_proposals(proposals, cls_scores.detach(), image.shape)\n        rpn_output = {\n            'proposals': proposals,\n            'scores': scores\n        }\n        if not self.training or target is None:\n            # If we are not training no need to do anything\n            return rpn_output\n        else:\n            # Assign gt box and label for each anchor\n            labels_for_anchors, matched_gt_boxes_for_anchors = self.assign_targets_to_anchors(\n                anchors,\n                target['bboxes'][0])\n\n            # Based on gt assignment above, get regression target for the anchors\n            # matched_gt_boxes_for_anchors -> (Number of anchors in image, 4)\n            # anchors -> (Number of anchors in image, 4)\n            regression_targets = boxes_to_transformation_targets(matched_gt_boxes_for_anchors, anchors)\n\n            ####### Sampling positive and negative anchors ####\n            # Our labels were {fg:1, bg:0, to_be_ignored:-1}\n            sampled_neg_idx_mask, sampled_pos_idx_mask = sample_positive_negative(\n                labels_for_anchors,\n                positive_count=self.rpn_pos_count,\n                total_count=self.rpn_batch_size)\n\n            sampled_idxs = torch.where(sampled_pos_idx_mask | sampled_neg_idx_mask)[0]\n\n            localization_loss = (\n                    torch.nn.functional.smooth_l1_loss(\n                        box_transform_pred[sampled_pos_idx_mask],\n                        regression_targets[sampled_pos_idx_mask],\n                        beta=1 / 9,\n                        reduction=\"sum\",\n                    )\n                    / (sampled_idxs.numel())\n            )\n\n            cls_loss = torch.nn.functional.binary_cross_entropy_with_logits(cls_scores[sampled_idxs].flatten(),\n                                                                            labels_for_anchors[sampled_idxs].flatten())\n\n            rpn_output['rpn_classification_loss'] = cls_loss\n            rpn_output['rpn_localization_loss'] = localization_loss\n            return rpn_output","metadata":{"id":"2eXiM1u9RlaA","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T14:10:21.170898Z","iopub.execute_input":"2025-01-09T14:10:21.171199Z","iopub.status.idle":"2025-01-09T14:10:21.192712Z","shell.execute_reply.started":"2025-01-09T14:10:21.171175Z","shell.execute_reply":"2025-01-09T14:10:21.192025Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class ROIHead(nn.Module):\n    r\"\"\"\n    ROI head on top of ROI pooling layer for generating\n    classification and box transformation predictions\n    We have two fc layers followed by a classification fc layer\n    and a bbox regression fc layer\n    \"\"\"\n\n    def __init__(self, model_config, num_classes, in_channels):\n        super(ROIHead, self).__init__()\n        self.num_classes = num_classes\n        self.roi_batch_size = model_config['roi_batch_size']\n        self.roi_pos_count = int(model_config['roi_pos_fraction'] * self.roi_batch_size)\n        self.iou_threshold = model_config['roi_iou_threshold']\n        self.low_bg_iou = model_config['roi_low_bg_iou']\n        self.nms_threshold = model_config['roi_nms_threshold']\n        self.topK_detections = model_config['roi_topk_detections']\n        self.low_score_threshold = model_config['roi_score_threshold']\n        self.pool_size = model_config['roi_pool_size']\n        self.fc_inner_dim = model_config['fc_inner_dim']\n\n        self.fc6 = nn.Linear(in_channels * self.pool_size * self.pool_size, self.fc_inner_dim)\n        self.fc7 = nn.Linear(self.fc_inner_dim, self.fc_inner_dim)\n        self.cls_layer = nn.Linear(self.fc_inner_dim, self.num_classes)\n        self.bbox_reg_layer = nn.Linear(self.fc_inner_dim, self.num_classes * 4)\n\n        torch.nn.init.normal_(self.cls_layer.weight, std=0.01)\n        torch.nn.init.constant_(self.cls_layer.bias, 0)\n\n        torch.nn.init.normal_(self.bbox_reg_layer.weight, std=0.001)\n        torch.nn.init.constant_(self.bbox_reg_layer.bias, 0)\n\n    def assign_target_to_proposals(self, proposals, gt_boxes, gt_labels):\n        r\"\"\"\n        Given a set of proposals and ground truth boxes and their respective labels.\n        Use IOU to assign these proposals to some gt box or background\n        :param proposals: (number_of_proposals, 4)\n        :param gt_boxes: (number_of_gt_boxes, 4)\n        :param gt_labels: (number_of_gt_boxes)\n        :return:\n            labels: (number_of_proposals)\n            matched_gt_boxes: (number_of_proposals, 4)\n        \"\"\"\n        # Get IOU Matrix between gt boxes and proposals\n        iou_matrix = get_iou(gt_boxes, proposals)\n        # For each gt box proposal find best matching gt box\n        best_match_iou, best_match_gt_idx = iou_matrix.max(dim=0)\n        background_proposals = (best_match_iou < self.iou_threshold) & (best_match_iou >= self.low_bg_iou)\n        ignored_proposals = best_match_iou < self.low_bg_iou\n\n        # Update best match of low IOU proposals to -1\n        best_match_gt_idx[background_proposals] = -1\n        best_match_gt_idx[ignored_proposals] = -2\n\n        # Get best marching gt boxes for ALL proposals\n        # Even background proposals would have a gt box assigned to it\n        # Label will be used to ignore them later\n        matched_gt_boxes_for_proposals = gt_boxes[best_match_gt_idx.clamp(min=0)]\n\n        # Get class label for all proposals according to matching gt boxes\n        labels = gt_labels[best_match_gt_idx.clamp(min=0)]\n        labels = labels.to(dtype=torch.int64)\n\n        # Update background proposals to be of label 10(background)\n        labels[background_proposals] = 0\n\n        # Set all to be ignored anchor labels as -1(will be ignored)\n        labels[ignored_proposals] = -1\n\n        return labels, matched_gt_boxes_for_proposals\n\n    def forward(self, feat, proposals, image_shape, target):\n        r\"\"\"\n        Main method for ROI head that does the following:\n        1. If training assign target boxes and labels to all proposals\n        2. If training sample positive and negative proposals\n        3. If training get bbox transformation targets for all proposals based on assignments\n        4. Get ROI Pooled features for all proposals\n        5. Call fc6, fc7 and classification and bbox transformation fc layers\n        6. Compute classification and localization loss\n\n        :param feat:\n        :param proposals:\n        :param image_shape:\n        :param target:\n        :return:\n        \"\"\"\n        if self.training and target is not None:\n            # Add ground truth to proposals\n            proposals = torch.cat([proposals, target['bboxes'][0]], dim=0)\n\n            gt_boxes = target['bboxes'][0]\n            gt_labels = target['labels'][0]\n\n            labels, matched_gt_boxes_for_proposals = self.assign_target_to_proposals(proposals, gt_boxes, gt_labels)\n\n            sampled_neg_idx_mask, sampled_pos_idx_mask = sample_positive_negative(labels,\n                                                                                  positive_count=self.roi_pos_count,\n                                                                                  total_count=self.roi_batch_size)\n\n            sampled_idxs = torch.where(sampled_pos_idx_mask | sampled_neg_idx_mask)[0]\n\n            # Keep only sampled proposals\n            proposals = proposals[sampled_idxs]\n            labels = labels[sampled_idxs]\n            matched_gt_boxes_for_proposals = matched_gt_boxes_for_proposals[sampled_idxs]\n            regression_targets = boxes_to_transformation_targets(matched_gt_boxes_for_proposals, proposals)\n            # regression_targets -> (sampled_training_proposals, 4)\n            # matched_gt_boxes_for_proposals -> (sampled_training_proposals, 4)\n\n        # Get desired scale to pass to roi pooling function\n        # For vgg16 case this would be 1/16 (0.0625)\n        size = feat.shape[-2:]\n        possible_scales = []\n        for s1, s2 in zip(size, image_shape):\n            approx_scale = float(s1) / float(s2)\n            scale = 2 ** float(torch.tensor(approx_scale).log2().round())\n            possible_scales.append(scale)\n        assert possible_scales[0] == possible_scales[1]\n\n        # ROI pooling and call all layers for prediction\n        proposal_roi_pool_feats = torchvision.ops.roi_pool(feat, [proposals],\n                                                           output_size=self.pool_size,\n                                                           spatial_scale=possible_scales[0])\n        proposal_roi_pool_feats = proposal_roi_pool_feats.flatten(start_dim=1)\n        box_fc_6 = torch.nn.functional.relu(self.fc6(proposal_roi_pool_feats))\n        box_fc_7 = torch.nn.functional.relu(self.fc7(box_fc_6))\n        cls_scores = self.cls_layer(box_fc_7)\n        box_transform_pred = self.bbox_reg_layer(box_fc_7)\n        # cls_scores -> (proposals, num_classes)\n        # box_transform_pred -> (proposals, num_classes * 4)\n        ##############################################\n\n        num_boxes, num_classes = cls_scores.shape\n        box_transform_pred = box_transform_pred.reshape(num_boxes, num_classes, 4)\n        frcnn_output = {}\n        if self.training and target is not None:\n            classification_loss = torch.nn.functional.cross_entropy(cls_scores, labels)\n\n            # Compute localization loss only for non-background labelled proposals\n            fg_proposals_idxs = torch.where((labels >= 0))[0]\n            # Get class labels for these positive proposals\n            fg_cls_labels = labels[fg_proposals_idxs]\n\n            localization_loss = torch.nn.functional.smooth_l1_loss(\n                box_transform_pred[fg_proposals_idxs, fg_cls_labels],\n                regression_targets[fg_proposals_idxs],\n                beta=1/9,\n                reduction=\"sum\",\n            )\n            localization_loss = localization_loss / labels.numel()\n            frcnn_output['frcnn_classification_loss'] = classification_loss\n            frcnn_output['frcnn_localization_loss'] = localization_loss\n\n        if self.training:\n            return frcnn_output\n        else:\n            device = cls_scores.device\n            # Apply transformation predictions to proposals\n            pred_boxes = apply_regression_pred_to_anchors_or_proposals(box_transform_pred, proposals)\n            pred_scores = torch.nn.functional.softmax(cls_scores, dim=-1)\n\n            # Clamp box to image boundary\n            pred_boxes = clamp_boxes_to_image_boundary(pred_boxes, image_shape)\n\n            # create labels for each prediction\n            pred_labels = torch.arange(num_classes, device=device)\n            pred_labels = pred_labels.view(1, -1).expand_as(pred_scores)\n\n            # remove predictions with the background label\n            pred_boxes = pred_boxes[:, 1:]\n            pred_scores = pred_scores[:, 1:]\n            pred_labels = pred_labels[:, 1:]\n\n            # pred_boxes -> (number_proposals, num_classes-1, 4)\n            # pred_scores -> (number_proposals, num_classes-1)\n            # pred_labels -> (number_proposals, num_classes-1)\n\n            # batch everything, by making every class prediction be a separate instance\n            pred_boxes = pred_boxes.reshape(-1, 4)\n            pred_scores = pred_scores.reshape(-1)\n            pred_labels = pred_labels.reshape(-1)\n\n            pred_boxes, pred_labels, pred_scores = self.filter_predictions(pred_boxes, pred_labels, pred_scores)\n            frcnn_output['boxes'] = pred_boxes\n            frcnn_output['scores'] = pred_scores\n            frcnn_output['labels'] = pred_labels\n            return frcnn_output\n\n    def filter_predictions(self, pred_boxes, pred_labels, pred_scores):\n        r\"\"\"\n        Method to filter predictions by applying the following in order:\n        1. Filter low scoring boxes\n        2. Remove small size boxes∂\n        3. NMS for each class separately\n        4. Keep only topK detections\n        :param pred_boxes:\n        :param pred_labels:\n        :param pred_scores:\n        :return:\n        \"\"\"\n        # remove low scoring boxes\n        keep = torch.where(pred_scores > self.low_score_threshold)[0]\n        pred_boxes, pred_scores, pred_labels = pred_boxes[keep], pred_scores[keep], pred_labels[keep]\n\n        # Remove small boxes\n        min_size = 16\n        ws, hs = pred_boxes[:, 2] - pred_boxes[:, 0], pred_boxes[:, 3] - pred_boxes[:, 1]\n        keep = (ws >= min_size) & (hs >= min_size)\n        keep = torch.where(keep)[0]\n        pred_boxes, pred_scores, pred_labels = pred_boxes[keep], pred_scores[keep], pred_labels[keep]\n\n        # Class wise nms\n        keep_mask = torch.zeros_like(pred_scores, dtype=torch.bool)\n        for class_id in torch.unique(pred_labels):\n            curr_indices = torch.where(pred_labels == class_id)[0]\n            curr_keep_indices = torch.ops.torchvision.nms(pred_boxes[curr_indices],\n                                                          pred_scores[curr_indices],\n                                                          self.nms_threshold)\n            keep_mask[curr_indices[curr_keep_indices]] = True\n        keep_indices = torch.where(keep_mask)[0]\n        post_nms_keep_indices = keep_indices[pred_scores[keep_indices].sort(descending=True)[1]]\n        keep = post_nms_keep_indices[:self.topK_detections]\n        pred_boxes, pred_scores, pred_labels = pred_boxes[keep], pred_scores[keep], pred_labels[keep]\n        return pred_boxes, pred_labels, pred_scores","metadata":{"id":"A4RZS7fpRlaC","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T14:10:21.591954Z","iopub.execute_input":"2025-01-09T14:10:21.592288Z","iopub.status.idle":"2025-01-09T14:10:21.615455Z","shell.execute_reply.started":"2025-01-09T14:10:21.592261Z","shell.execute_reply":"2025-01-09T14:10:21.614468Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"\nclass VGG16Backbone(nn.Module):\n    def __init__(self):\n        super(VGG16Backbone, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n        )\n        # Initialiser les poids avec une distribution normale ou uniforme\n        self._initialize_weights()\n\n    def forward(self, x):\n        return self.features(x)\n\n    def _initialize_weights(self):\n        for m in self.features:\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T14:10:22.186398Z","iopub.execute_input":"2025-01-09T14:10:22.186737Z","iopub.status.idle":"2025-01-09T14:10:22.194189Z","shell.execute_reply.started":"2025-01-09T14:10:22.186713Z","shell.execute_reply":"2025-01-09T14:10:22.193309Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class FasterRCNN(nn.Module):\n    def __init__(self, model_config, num_classes):\n        super(FasterRCNN, self).__init__()\n        self.model_config = model_config\n        vgg16_backbone = VGG16Backbone()\n        self.backbone = vgg16_backbone.features[:-1]\n        self.rpn = RegionProposalNetwork(model_config['backbone_out_channels'],\n                                         scales=model_config['scales'],\n                                         aspect_ratios=model_config['aspect_ratios'],\n                                         model_config=model_config)\n        self.roi_head = ROIHead(model_config, num_classes, in_channels=model_config['backbone_out_channels'])\n        \n        self.image_mean = [0.485, 0.456, 0.406]\n        self.image_std = [0.229, 0.224, 0.225]\n        self.min_size = model_config['min_im_size']\n        self.max_size = model_config['max_im_size']\n\n    def normalize_resize_image_and_boxes(self, image, bboxes):\n        dtype, device = image.dtype, image.device\n\n        # Normalize\n        mean = torch.as_tensor(self.image_mean, dtype=dtype, device=device)\n        std = torch.as_tensor(self.image_std, dtype=dtype, device=device)\n        image = (image - mean[:, None, None]) / std[:, None, None]\n        #############\n\n        # Resize to 1000x600 such that lowest size dimension is scaled upto 600\n        # but larger dimension is not more than 1000\n        # So compute scale factor for both and scale is minimum of these two\n        h, w = image.shape[-2:]\n        im_shape = torch.tensor(image.shape[-2:])\n        min_size = torch.min(im_shape).to(dtype=torch.float32)\n        max_size = torch.max(im_shape).to(dtype=torch.float32)\n        scale = torch.min(float(self.min_size) / min_size, float(self.max_size) / max_size)\n        scale_factor = scale.item()\n\n        # Resize image based on scale computed\n        image = torch.nn.functional.interpolate(\n            image,\n            size=None,\n            scale_factor=scale_factor,\n            mode=\"bilinear\",\n            recompute_scale_factor=True,\n            align_corners=False,\n        )\n\n        if bboxes is not None:\n            # Resize boxes by\n            ratios = [\n                torch.tensor(s, dtype=torch.float32, device=bboxes.device)\n                / torch.tensor(s_orig, dtype=torch.float32, device=bboxes.device)\n                for s, s_orig in zip(image.shape[-2:], (h, w))\n            ]\n            ratio_height, ratio_width = ratios\n            xmin, ymin, xmax, ymax = bboxes.unbind(2)\n            xmin = xmin * ratio_width\n            xmax = xmax * ratio_width\n            ymin = ymin * ratio_height\n            ymax = ymax * ratio_height\n            bboxes = torch.stack((xmin, ymin, xmax, ymax), dim=2)\n        return image, bboxes\n\n    def forward(self, image, target=None):\n        old_shape = image.shape[-2:]\n        if self.training:\n            # Normalize and resize boxes\n            image, bboxes = self.normalize_resize_image_and_boxes(image, target['bboxes'])\n            target['bboxes'] = bboxes\n        else:\n            image, _ = self.normalize_resize_image_and_boxes(image, None)\n\n        # Call backbone\n        feat = self.backbone(image)\n\n        # Call RPN and get proposals\n        rpn_output = self.rpn(image, feat, target)\n        proposals = rpn_output['proposals']\n\n        # Call ROI head and convert proposals to boxes\n        frcnn_output = self.roi_head(feat, proposals, image.shape[-2:], target)\n        if not self.training:\n            # Transform boxes to original image dimensions called only during inference\n            frcnn_output['boxes'] = transform_boxes_to_original_size(frcnn_output['boxes'],\n                                                                     image.shape[-2:],\n                                                                     old_shape)\n        return rpn_output, frcnn_output\n","metadata":{"id":"wjzBo1ZBRlaD","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T14:10:35.448357Z","iopub.execute_input":"2025-01-09T14:10:35.448690Z","iopub.status.idle":"2025-01-09T14:10:35.459611Z","shell.execute_reply.started":"2025-01-09T14:10:35.448660Z","shell.execute_reply":"2025-01-09T14:10:35.458380Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Détecter si CUDA est disponible et utiliser le GPU, sinon utiliser le CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T14:10:36.305410Z","iopub.execute_input":"2025-01-09T14:10:36.305701Z","iopub.status.idle":"2025-01-09T14:10:36.356016Z","shell.execute_reply.started":"2025-01-09T14:10:36.305680Z","shell.execute_reply":"2025-01-09T14:10:36.355112Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import torch\nimport os\nimport numpy as np\nimport random\nfrom tqdm import tqdm\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.optim.lr_scheduler import MultiStepLR\n\n# Détecter si CUDA est disponible et utiliser le GPU, sinon utiliser le CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')\n\nconfig = {\n    'dataset_params': {\n        'im_train_path': '/kaggle/input/official/Officiel/train/images',\n        'ann_train_path': '/kaggle/input/official/Officiel/train/csv',\n        'im_test_path': '/kaggle/input/official/Officiel/test/images',\n        'ann_test_path': '/kaggle/input/official/Officiel/test/csv',\n        'classes' : ['Maize', 'Sugar beet', 'Soy', 'Sunflower', 'Potato', 'Pea', 'Bean', 'Pumpkin', 'Weed'],\n        'num_classes': 10\n    },\n    'model_params': {\n        'im_channels': 3,\n        'aspect_ratios': [0.5, 1, 2],\n        'scales': [128, 256, 512],\n        'min_im_size': 600,\n        'max_im_size': 800,\n        'backbone_out_channels': 512,\n        'fc_inner_dim': 1024,\n        'rpn_bg_threshold': 0.3,\n        'rpn_fg_threshold': 0.7,\n        'rpn_nms_threshold': 0.7,\n        'rpn_train_prenms_topk': 6000,\n        'rpn_test_prenms_topk': 3000,\n        'rpn_train_topk': 2000,\n        'rpn_test_topk': 300,\n        'rpn_batch_size': 256,\n        'rpn_pos_fraction': 0.5,\n        'roi_iou_threshold': 0.5,\n        'roi_low_bg_iou': 0.0,\n        'roi_pool_size': 7,\n        'roi_nms_threshold': 0.3,\n        'roi_topk_detections': 100,\n        'roi_score_threshold': 0.05,\n        'roi_batch_size': 128,\n        'roi_pos_fraction': 0.25\n    },\n    'train_params': {\n        'task_name': 'voc',\n        'seed': 1111,\n        'acc_steps': 15,\n        'num_epochs': 15,\n        'lr_steps': [12, 16],\n        'lr': 0.001,\n        'ckpt_name': 'faster_rcnn.pth'\n    }\n}\n\n# Extraire les paramètres\ndataset_config = config['dataset_params']\nmodel_config = config['model_params']\ntrain_config = config['train_params']\n\n# Initialiser la graine aléatoire pour la reproductibilité\nseed = train_config['seed']\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\n\nif device == 'cuda':\n        torch.cuda.manual_seed_all(seed)\n\n# Charger le dataset\ndetection_data = CSVDetectionDataset(\n    'train',\n    im_dir=dataset_config['im_train_path'],\n    ann_dir=dataset_config['ann_train_path'],\n    classes = dataset_config['classes']\n)\ntrain_dataset = DataLoader(\n    detection_data,\n    batch_size=1,\n    shuffle=True,\n    num_workers=4\n)\n\n# Initialiser le modèle Faster R-CNN\nfaster_rcnn_model = FasterRCNN(\n    model_config,\n    num_classes=dataset_config['num_classes']\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T14:10:37.218261Z","iopub.execute_input":"2025-01-09T14:10:37.218563Z","iopub.status.idle":"2025-01-09T14:11:21.919702Z","shell.execute_reply.started":"2025-01-09T14:10:37.218540Z","shell.execute_reply":"2025-01-09T14:11:21.918884Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n{0: 'background', 1: 'Maize', 2: 'Sugar beet', 3: 'Soy', 4: 'Sunflower', 5: 'Potato', 6: 'Pea', 7: 'Bean', 8: 'Pumpkin', 9: 'Weed'}\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 6427/6427 [00:44<00:00, 145.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Total 6233 images with annotations found.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"faster_rcnn_model.train()\nfaster_rcnn_model.to(device)  # Charger le modèle sur le périphérique (CPU ou GPU)\n\n# Préparer le dossier pour les checkpoints\nif not os.path.exists(train_config['task_name']):\n    os.mkdir(train_config['task_name'])\n\n# Configurer l'optimiseur et le scheduler\noptimizer = torch.optim.SGD(\n    lr=train_config['lr'],\n    params=filter(lambda p: p.requires_grad, faster_rcnn_model.parameters()),\n    weight_decay=5E-4,\n    momentum=0.9\n)\nscheduler = MultiStepLR(optimizer, milestones=train_config['lr_steps'], gamma=0.1)\n\n# Paramètres d'entraînement\nacc_steps = train_config['acc_steps']\nnum_epochs = train_config['num_epochs']\nstep_count = 1\n\n# Boucle d'entraînement\nfor i in range(num_epochs):\n    rpn_classification_losses = []\n    rpn_localization_losses = []\n    frcnn_classification_losses = []\n    frcnn_localization_losses = []\n    optimizer.zero_grad()\n\n    for im, target, fname in tqdm(train_dataset):\n        im = im.float().to(device)  # Déplacer les images vers le périphérique (CPU ou GPU)\n        target['bboxes'] = target['bboxes'].float().to(device)  # Déplacer les boîtes vers le périphérique\n        target['labels'] = target['labels'].long().to(device)  # Déplacer les labels vers le périphérique\n\n        rpn_output, frcnn_output = faster_rcnn_model(im, target)\n\n        rpn_loss = rpn_output['rpn_classification_loss'] + rpn_output['rpn_localization_loss']\n        frcnn_loss = frcnn_output['frcnn_classification_loss'] + frcnn_output['frcnn_localization_loss']\n        loss = rpn_loss + frcnn_loss\n\n        rpn_classification_losses.append(rpn_output['rpn_classification_loss'].item())\n        rpn_localization_losses.append(rpn_output['rpn_localization_loss'].item())\n        frcnn_classification_losses.append(frcnn_output['frcnn_classification_loss'].item())\n        frcnn_localization_losses.append(frcnn_output['frcnn_localization_loss'].item())\n\n        loss = loss / acc_steps\n        loss.backward()\n\n        if step_count % acc_steps == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n\n        step_count += 1\n\n    # Sauvegarde du modèle et affichage des pertes\n    print(f'Finished epoch {i}')\n    optimizer.step()\n    optimizer.zero_grad()\n    torch.save(\n        faster_rcnn_model.state_dict(),\n        os.path.join(train_config['task_name'], train_config['ckpt_name'])\n    )\n    loss_output = (\n        f'RPN Classification Loss : {np.mean(rpn_classification_losses):.4f} | '\n        f'RPN Localization Loss : {np.mean(rpn_localization_losses):.4f} | '\n        f'FRCNN Classification Loss : {np.mean(frcnn_classification_losses):.4f} | '\n        f'FRCNN Localization Loss : {np.mean(frcnn_localization_losses):.4f}'\n    )\n    print(loss_output)\n    scheduler.step()\n\nprint('Done Training...')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T05:05:57.373499Z","iopub.execute_input":"2025-01-09T05:05:57.373746Z","iopub.status.idle":"2025-01-09T07:28:21.286579Z","shell.execute_reply.started":"2025-01-09T05:05:57.373724Z","shell.execute_reply":"2025-01-09T07:28:21.285662Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 6233/6233 [09:22<00:00, 11.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 0\nRPN Classification Loss : 0.3171 | RPN Localization Loss : 0.7556 | FRCNN Classification Loss : 0.4797 | FRCNN Localization Loss : 9.2975\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 6233/6233 [09:26<00:00, 11.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 1\nRPN Classification Loss : 0.2262 | RPN Localization Loss : 0.5159 | FRCNN Classification Loss : 0.3509 | FRCNN Localization Loss : 6.3363\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 6233/6233 [09:29<00:00, 10.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 2\nRPN Classification Loss : 0.2062 | RPN Localization Loss : 0.4664 | FRCNN Classification Loss : 0.3202 | FRCNN Localization Loss : 6.0999\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 6233/6233 [09:27<00:00, 10.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 3\nRPN Classification Loss : 0.1951 | RPN Localization Loss : 0.4494 | FRCNN Classification Loss : 0.2964 | FRCNN Localization Loss : 5.9842\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 6233/6233 [09:30<00:00, 10.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 4\nRPN Classification Loss : 0.1887 | RPN Localization Loss : 0.4373 | FRCNN Classification Loss : 0.2822 | FRCNN Localization Loss : 5.9828\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 6233/6233 [09:31<00:00, 10.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 5\nRPN Classification Loss : 0.1827 | RPN Localization Loss : 0.4175 | FRCNN Classification Loss : 0.2692 | FRCNN Localization Loss : 5.8485\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 6233/6233 [09:31<00:00, 10.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 6\nRPN Classification Loss : 0.1804 | RPN Localization Loss : 0.4132 | FRCNN Classification Loss : 0.2598 | FRCNN Localization Loss : 5.7941\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 6233/6233 [09:32<00:00, 10.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 7\nRPN Classification Loss : 0.1742 | RPN Localization Loss : 0.4004 | FRCNN Classification Loss : 0.2489 | FRCNN Localization Loss : 5.8172\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 6233/6233 [09:32<00:00, 10.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 8\nRPN Classification Loss : 0.1726 | RPN Localization Loss : 0.3941 | FRCNN Classification Loss : 0.2469 | FRCNN Localization Loss : 5.8647\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 6233/6233 [09:28<00:00, 10.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 9\nRPN Classification Loss : 0.1693 | RPN Localization Loss : 0.3839 | FRCNN Classification Loss : 0.2367 | FRCNN Localization Loss : 5.6817\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 6233/6233 [09:29<00:00, 10.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 10\nRPN Classification Loss : 0.1662 | RPN Localization Loss : 0.3816 | FRCNN Classification Loss : 0.2308 | FRCNN Localization Loss : 5.7085\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 6233/6233 [09:29<00:00, 10.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 11\nRPN Classification Loss : 0.1646 | RPN Localization Loss : 0.3736 | FRCNN Classification Loss : 0.2233 | FRCNN Localization Loss : 5.7630\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 6233/6233 [09:26<00:00, 10.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 12\nRPN Classification Loss : 0.1570 | RPN Localization Loss : 0.3432 | FRCNN Classification Loss : 0.2031 | FRCNN Localization Loss : 5.7190\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 6233/6233 [09:26<00:00, 11.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 13\nRPN Classification Loss : 0.1561 | RPN Localization Loss : 0.3402 | FRCNN Classification Loss : 0.1974 | FRCNN Localization Loss : 5.7009\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 6233/6233 [09:31<00:00, 10.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 14\nRPN Classification Loss : 0.1553 | RPN Localization Loss : 0.3386 | FRCNN Classification Loss : 0.1963 | FRCNN Localization Loss : 5.7299\nDone Training...\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"def get_iou_eval(det, gt):\n    det_x1, det_y1, det_x2, det_y2 = det\n    gt_x1, gt_y1, gt_x2, gt_y2 = gt\n    \n    x_left = max(det_x1, gt_x1)\n    y_top = max(det_y1, gt_y1)\n    x_right = min(det_x2, gt_x2)\n    y_bottom = min(det_y2, gt_y2)\n    \n    if x_right < x_left or y_bottom < y_top:\n        return 0.0\n    \n    area_intersection = (x_right - x_left) * (y_bottom - y_top)\n    det_area = (det_x2 - det_x1) * (det_y2 - det_y1)\n    gt_area = (gt_x2 - gt_x1) * (gt_y2 - gt_y1)\n    area_union = float(det_area + gt_area - area_intersection + 1E-6)\n    iou = area_intersection / area_union\n    return iou\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"-MoKftrSRlaF","outputId":"367a7649-a7f6-442d-b4c4-c2aca8b323a8","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T07:28:21.288361Z","iopub.execute_input":"2025-01-09T07:28:21.288584Z","iopub.status.idle":"2025-01-09T07:28:21.293931Z","shell.execute_reply.started":"2025-01-09T07:28:21.288566Z","shell.execute_reply":"2025-01-09T07:28:21.292981Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def compute_map(det_boxes, gt_boxes, iou_threshold=0.5, method='area'):\n    gt_labels = {cls_key for im_gt in gt_boxes for cls_key in im_gt.keys()}\n    gt_labels = sorted(gt_labels)\n    all_aps = {}\n    aps = []\n    for idx, label in enumerate(gt_labels):\n        # Get detection predictions of this class\n        cls_dets = [\n            [im_idx, im_dets_label] for im_idx, im_dets in enumerate(det_boxes)\n            if label in im_dets for im_dets_label in im_dets[label]\n        ]\n        \n        cls_dets = sorted(cls_dets, key=lambda k: -k[1][-1])\n        \n        # For tracking which gt boxes of this class have already been matched\n        gt_matched = [[False for _ in im_gts[label]] for im_gts in gt_boxes]\n        # Number of gt boxes for this class for recall calculation\n        num_gts = sum([len(im_gts[label]) for im_gts in gt_boxes])\n        tp = [0] * len(cls_dets)\n        fp = [0] * len(cls_dets)\n        \n        # For each prediction\n        for det_idx, (im_idx, det_pred) in enumerate(cls_dets):\n            # Get gt boxes for this image and this label\n            im_gts = gt_boxes[im_idx][label]\n            max_iou_found = -1\n            max_iou_gt_idx = -1\n            \n            # Get best matching gt box\n            for gt_box_idx, gt_box in enumerate(im_gts):\n                gt_box_iou = get_iou_eval(det_pred[:-1], gt_box)\n                if gt_box_iou > max_iou_found:\n                    max_iou_found = gt_box_iou\n                    max_iou_gt_idx = gt_box_idx\n            # TP only if iou >= threshold and this gt has not yet been matched\n            if max_iou_found < iou_threshold or gt_matched[im_idx][max_iou_gt_idx]:\n                fp[det_idx] = 1\n            else:\n                tp[det_idx] = 1\n                # If tp then we set this gt box as matched\n                gt_matched[im_idx][max_iou_gt_idx] = True\n        # Cumulative tp and fp\n        tp = np.cumsum(tp)\n        fp = np.cumsum(fp)\n        \n        eps = np.finfo(np.float32).eps\n        recalls = tp / np.maximum(num_gts, eps)\n        precisions = tp / np.maximum((tp + fp), eps)\n\n        if method == 'area':\n            recalls = np.concatenate(([0.0], recalls, [1.0]))\n            precisions = np.concatenate(([0.0], precisions, [0.0]))\n            \n            # Replace precision values with recall r with maximum precision value\n            # of any recall value >= r\n            # This computes the precision envelope\n            for i in range(precisions.size - 1, 0, -1):\n                precisions[i - 1] = np.maximum(precisions[i - 1], precisions[i])\n            # For computing area, get points where recall changes value\n            i = np.where(recalls[1:] != recalls[:-1])[0]\n            # Add the rectangular areas to get ap\n            ap = np.sum((recalls[i + 1] - recalls[i]) * precisions[i + 1])\n        elif method == 'interp':\n            ap = 0.0\n            for interp_pt in np.arange(0, 1 + 1E-3, 0.1):\n                # Get precision values for recall values >= interp_pt\n                prec_interp_pt = precisions[recalls >= interp_pt]\n                \n                # Get max of those precision values\n                prec_interp_pt = prec_interp_pt.max() if prec_interp_pt.size > 0.0 else 0.0\n                ap += prec_interp_pt\n            ap = ap / 11.0\n        else:\n            raise ValueError('Method can only be area or interp')\n        if num_gts > 0:\n            aps.append(ap)\n            all_aps[label] = ap\n        else:\n            all_aps[label] = np.nan\n    # compute mAP at provided iou threshold\n    mean_ap = sum(aps) / len(aps)\n    return mean_ap, all_aps\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T07:28:21.295004Z","iopub.execute_input":"2025-01-09T07:28:21.295237Z","iopub.status.idle":"2025-01-09T07:28:21.310815Z","shell.execute_reply.started":"2025-01-09T07:28:21.295219Z","shell.execute_reply":"2025-01-09T07:28:21.309880Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def load_model_and_dataset():\n    \n    dataset_config = config['dataset_params']\n    model_config = config['model_params']\n    train_config = config['train_params']\n    \n    seed = train_config['seed']\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    if device == 'cuda':\n        torch.cuda.manual_seed_all(seed)\n    \n    voc = CSVDetectionDataset('test', im_dir=dataset_config['im_test_path'], ann_dir=dataset_config['ann_test_path'], classes = dataset_config['classes'])\n    test_dataset = DataLoader(voc, batch_size=1, shuffle=False)\n\n    # Créez et configurez votre modèle\n    faster_rcnn_model = FasterRCNN(model_config, num_classes=dataset_config['num_classes'])\n    \n    # Mettre le modèle en mode évaluation\n    faster_rcnn_model.eval()\n    \n    # Déplacer le modèle sur le bon appareil (CPU ou GPU)\n    faster_rcnn_model.to(device)\n    \n    # Charger les poids du modèle en toute sécurité avec 'weights_only=True'\n    checkpoint = torch.load('/kaggle/working/voc/faster_rcnn.pth', map_location=device, weights_only=True)\n    \n    # Charger les poids dans le modèle\n    faster_rcnn_model.load_state_dict(checkpoint)\n\n\n\n    return faster_rcnn_model, voc, test_dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T07:28:21.312125Z","iopub.execute_input":"2025-01-09T07:28:21.312406Z","iopub.status.idle":"2025-01-09T07:28:21.325621Z","shell.execute_reply.started":"2025-01-09T07:28:21.312386Z","shell.execute_reply":"2025-01-09T07:28:21.324828Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import os\nimport random\nimport cv2\nfrom tqdm import tqdm\n\ndef infer():\n    if not os.path.exists('samples'):\n        os.mkdir('samples')\n    \n    # Charger le modèle et les données\n    faster_rcnn_model, voc, test_dataset = load_model_and_dataset()\n\n    # Hard coding du seuil de score faible pour l'inférence sur les images pour le moment\n    # Devrait provenir de la configuration\n    faster_rcnn_model.roi_head.low_score_threshold = 0.7\n    \n    for sample_count in tqdm(range(10)):\n        random_idx = random.randint(0, len(voc) - 1)\n        im, target, fname = voc[random_idx]\n        im = im.unsqueeze(0).float().to(device)\n\n        gt_im = cv2.imread(fname)\n        gt_im_copy = gt_im.copy()\n\n        # Sauvegarder les images avec les boîtes de vérité terrain\n        for idx, box in enumerate(target['bboxes']):\n            label_idx = target['labels'][idx].detach().cpu().item()\n            \n            # Ignorer les boîtes dont le label est -1 (background)\n            if label_idx == -1:\n                continue  # Passer à l'itération suivante\n\n            x1, y1, x2, y2 = box.detach().cpu().numpy()\n            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n\n            cv2.rectangle(gt_im, (x1, y1), (x2, y2), thickness=2, color=[0, 255, 0])\n            cv2.rectangle(gt_im_copy, (x1, y1), (x2, y2), thickness=2, color=[0, 255, 0])\n\n            text = voc.idx2label[label_idx]  # Accéder au label valide\n\n            # Calculer la taille du texte et afficher le label\n            text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_PLAIN, 1, 1)\n            text_w, text_h = text_size\n            cv2.rectangle(gt_im_copy, (x1, y1), (x1 + 10 + text_w, y1 + 10 + text_h), [255, 255, 255], -1)\n            cv2.putText(gt_im, text=text,\n                        org=(x1 + 5, y1 + 15),\n                        thickness=1,\n                        fontScale=1,\n                        color=[0, 0, 0],\n                        fontFace=cv2.FONT_HERSHEY_PLAIN)\n            cv2.putText(gt_im_copy, text=text,\n                        org=(x1 + 5, y1 + 15),\n                        thickness=1,\n                        fontScale=1,\n                        color=[0, 0, 0],\n                        fontFace=cv2.FONT_HERSHEY_PLAIN)\n\n        # Superposer les images avec un certain poids\n        cv2.addWeighted(gt_im_copy, 0.7, gt_im, 0.3, 0, gt_im)\n        cv2.imwrite('samples/output_frcnn_gt_{}.png'.format(sample_count), gt_im)\n        \n        # Récupérer les prédictions du modèle entraîné\n        rpn_output, frcnn_output = faster_rcnn_model(im, None)\n        boxes = frcnn_output['boxes']\n        labels = frcnn_output['labels']\n        scores = frcnn_output['scores']\n        \n        # Charger l'image de l'entrée\n        im = cv2.imread(fname)\n        im_copy = im.copy()\n\n        # Sauvegarder les images avec les boîtes de prédiction\n        for idx, box in enumerate(boxes):\n            x1, y1, x2, y2 = box.detach().cpu().numpy()\n            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n\n            cv2.rectangle(im, (x1, y1), (x2, y2), thickness=2, color=[0, 0, 255])\n            cv2.rectangle(im_copy, (x1, y1), (x2, y2), thickness=2, color=[0, 0, 255])\n\n            label_idx = labels[idx].detach().cpu().item()\n            score = scores[idx].detach().cpu().item()\n\n            # Ignorer les boîtes avec un score trop faible ou le label -1\n            if label_idx == -1 or score < 0.3:\n                continue  # Passer à l'itération suivante\n\n            text = '{} : {:.2f}'.format(voc.idx2label[label_idx], score)\n\n            # Calculer la taille du texte et afficher le label\n            text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_PLAIN, 1, 1)\n            text_w, text_h = text_size\n            cv2.rectangle(im_copy, (x1, y1), (x1 + 10 + text_w, y1 + 10 + text_h), [255, 255, 255], -1)\n            cv2.putText(im, text=text,\n                        org=(x1 + 5, y1 + 15),\n                        thickness=1,\n                        fontScale=1,\n                        color=[0, 0, 0],\n                        fontFace=cv2.FONT_HERSHEY_PLAIN)\n            cv2.putText(im_copy, text=text,\n                        org=(x1 + 5, y1 + 15),\n                        thickness=1,\n                        fontScale=1,\n                        color=[0, 0, 0],\n                        fontFace=cv2.FONT_HERSHEY_PLAIN)\n\n        # Superposer les images avec un certain poids\n        cv2.addWeighted(im_copy, 0.7, im, 0.3, 0, im)\n        cv2.imwrite('samples/output_frcnn_{}.jpg'.format(sample_count), im)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T07:36:28.309614Z","iopub.execute_input":"2025-01-09T07:36:28.309960Z","iopub.status.idle":"2025-01-09T07:36:28.323709Z","shell.execute_reply.started":"2025-01-09T07:36:28.309927Z","shell.execute_reply":"2025-01-09T07:36:28.322889Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def evaluate_map():\n    faster_rcnn_model, voc, test_dataset = load_model_and_dataset()\n    gts = []\n    preds = []\n    \n    # Initialiser les dictionnaires pour ne pas inclure 'background'\n    for im, target, fname in tqdm(test_dataset):\n        im_name = fname\n        im = im.float().to(device)\n        target_boxes = target['bboxes'].float().to(device)[0]\n        target_labels = target['labels'].long().to(device)[0]\n        rpn_output, frcnn_output = faster_rcnn_model(im, None)\n\n        boxes = frcnn_output['boxes']\n        labels = frcnn_output['labels']\n        scores = frcnn_output['scores']\n        \n        # Initialisation des boîtes de prédiction et de vérité, sans 'background'\n        pred_boxes = {}\n        gt_boxes = {}\n        for label_name in voc.label2idx:\n            if label_name != 'background':  # Ne pas inclure 'background' dans les dictionnaires\n                pred_boxes[label_name] = []\n                gt_boxes[label_name] = []\n\n        # Boucle pour les prédictions\n        for idx, box in enumerate(boxes):\n            x1, y1, x2, y2 = box.detach().cpu().numpy()\n            label = labels[idx].detach().cpu().item()\n            score = scores[idx].detach().cpu().item()\n            \n            # Gestion des labels invalides (par exemple, -1)\n            if label >= 0 and label < len(voc.idx2label):\n                label_name = voc.idx2label[label]\n            else:\n                label_name = 'background'  # Remplacer par 'background' si le label est invalide\n                \n            # Ne pas ajouter 'background' aux prédictions\n            if label_name != 'background':\n                pred_boxes[label_name].append([x1, y1, x2, y2, score])\n        \n        # Boucle pour les vérités terrain (ground truth)\n        for idx, box in enumerate(target_boxes):\n            x1, y1, x2, y2 = box.detach().cpu().numpy()\n            label = target_labels[idx].detach().cpu().item()\n            \n            # Gestion des labels invalides (par exemple, -1)\n            if label >= 0 and label < len(voc.idx2label):\n                label_name = voc.idx2label[label]\n            else:\n                label_name = 'background'  # Remplacer par 'background' si le label est invalide\n                \n            # Ne pas ajouter 'background' aux vérités terrain\n            if label_name != 'background':\n                gt_boxes[label_name].append([x1, y1, x2, y2])\n        \n        gts.append(gt_boxes)\n        preds.append(pred_boxes)\n   \n    # Calcul de la mAP\n    mean_ap, all_aps = compute_map(preds, gts, method='interp')\n    \n    print('Class Wise Average Precisions (excluding background)')\n    for idx in range(len(voc.idx2label)):\n        label_name = voc.idx2label[idx]\n        if label_name != 'background':  # Ne pas inclure 'background' dans l'affichage\n            print('AP for class {} = {:.4f}'.format(label_name, all_aps[label_name]))\n        \n    print('Mean Average Precision (excluding background): {:.4f}'.format(mean_ap))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T07:28:21.642215Z","iopub.execute_input":"2025-01-09T07:28:21.642512Z","iopub.status.idle":"2025-01-09T07:28:21.651596Z","shell.execute_reply.started":"2025-01-09T07:28:21.642482Z","shell.execute_reply":"2025-01-09T07:28:21.650665Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"\nimport cv2\n\nevaluate_map()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T07:28:21.653451Z","iopub.execute_input":"2025-01-09T07:28:21.653715Z","iopub.status.idle":"2025-01-09T07:32:31.258749Z","shell.execute_reply.started":"2025-01-09T07:28:21.653681Z","shell.execute_reply":"2025-01-09T07:32:31.257896Z"}},"outputs":[{"name":"stdout","text":"{0: 'background', 1: 'Maize', 2: 'Sugar beet', 3: 'Soy', 4: 'Sunflower', 5: 'Potato', 6: 'Pea', 7: 'Bean', 8: 'Pumpkin', 9: 'Weed'}\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1607/1607 [00:15<00:00, 106.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Total 1567 images with annotations found.\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1567/1567 [03:51<00:00,  6.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Class Wise Average Precisions (excluding background)\nAP for class Maize = 0.6716\nAP for class Sugar beet = 0.5586\nAP for class Soy = 0.2015\nAP for class Sunflower = 0.5707\nAP for class Potato = 0.4347\nAP for class Pea = 0.3734\nAP for class Bean = 0.4138\nAP for class Pumpkin = 0.7474\nAP for class Weed = 0.1822\nMean Average Precision (excluding background): 0.4615\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"infer()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T07:36:40.562013Z","iopub.execute_input":"2025-01-09T07:36:40.562312Z","iopub.status.idle":"2025-01-09T07:36:50.367058Z","shell.execute_reply.started":"2025-01-09T07:36:40.562292Z","shell.execute_reply":"2025-01-09T07:36:50.366189Z"}},"outputs":[{"name":"stdout","text":"{0: 'background', 1: 'Maize', 2: 'Sugar beet', 3: 'Soy', 4: 'Sunflower', 5: 'Potato', 6: 'Pea', 7: 'Bean', 8: 'Pumpkin', 9: 'Weed'}\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1607/1607 [00:04<00:00, 381.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Total 1567 images with annotations found.\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [00:03<00:00,  2.87it/s]\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}